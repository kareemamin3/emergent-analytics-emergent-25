{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Data preparation of NOTAMs\n\nWhat is NOTAMS?\n\nNOTAM actually stands for Notice To Airmen and is the primary means of disseminating all kinds of information to pilots\n\nAs NOTAMs contain critical information about aircraft and passenger entry requirements, we use NLP to extract relevant information from these messages. NOTAMs are similar to telegram messages and contain a lot of abbreviations to reduce the length of the message. So before extracting information from these message, the text has to be preprocessed.\n\nThis notebook deals with cleaning the NOTAMs messages that can be later used for further analysis.\n\n**NOTE:**\n\nThe results in this notebook cannot be directly reproduced. The input data has to be downloaded by the user!\n\nWe read in the NOTAMS data downloaded from https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx. The data will not be published on this site and has to be downloaded by each user on their own to carry out the following analysis. In order to download the data, the user must register on https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx to get a free API key.\n\n\n\n## Data Sources\n\n**Data collection from ICAO website**\n\nThe following data are collected:\n\n1. COVID related NOTAMS from airports (Airport COVID-19 NOTAMs)\n2. COVID related NOTAMS from airpspaces (Airspace COVID-19 NOTAMs)\n\nAirport NOTAMS also provide information about closure of airports\n\n\nExample of a COVID-19 NOTAM message:\n\n    COVID-19: ORDERS OF THE STATE GOVERNMENT OF BRANDENBURG WITH THE AIM OF PREVENTING THE INTRODUCTION OR SPREAD OF INFECTIONS BY SARS-COV-2. ALL PAX ENTERING THE FEDERAL REPUBLIC OF GERMANY AS THEIR FINAL DESTINATION FROM RISK AREAS DIRECT OR VIA TRANSFER(1) MUST STAY IN QUARANTINE FOR 14 DAYS AFTER ARRIVAL AND (2) MUST CONTACT LOCAL HEALTH AUTHORITY OF THEIR FINAL DESTINATION IMMEDIATLY. EXCEPTIONS ARE POSSIBLE IN THE CASE OF A NEGARIVE PCR TEST FOR SARS-COV-2 IN GERMAN AND ENGLISH FOR A MAXIMUM OF 48 HOURS BEFORE ENTRY. THESE REGULATIONS DO NOT APPLY FOR CREW MEMBERS. THE CREW MUST PROVIDE INFORMATION ABOUT THESE REGULATIONS TO ALL PAX INFLIGHT. CREATED: 15 Jun 2020 15:58:00 SOURCE: EUECYIYN\n\n\n**Input**\n\n   Downloaded datasets\n\n    - all_airports_covid_notams_xx.csv\n    - all_airspace_covid_notams_xx.csv\n\n**Output**\n  \n  Preprocessed datasets\n  \n    - valid_airport_notams_xx.csv\n    - valid_airspace_notams_xx.csv\n\nwhere 'xx' corresponds to the date\n\nThe following steps are carried out in preprocessing the data:\n\n1. Extracting NOTAMs from json: As the NOTAMs are stored in json format, the intial step is to extract these messages from the json string\n\n\n2. Cleaning the NOTAMS\n\n    * Remove white spaces\n    * Remove hyperlinks\n    * Mapping abbreviations to actual words\n    * Remove foreign text if the phrase \"english text/english version\" is present\n    * Remove words starting with symbols\n    * Remove punctuations\n\n\n3. Generating tokens\n\n    * Remove numbers\n    * Lemmatization\n    * Remove stop words"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "try:\n    import spacy\nexcept:\n    !pip install spacy\ntry:\n    import spacy_langdetect\nexcept:\n    !pip install spacy-langdetect\ntry:\n    import flair\nexcept:\n    !pip install flair\ntry:\n    import geonamescache\nexcept:\n    !pip install geonamescache\ntry:\n    import spacy_fastlang\nexcept:    \n    !pip install spacy_fastlang\n    #!pip install sense2vec==1.0.0a1\ntry:\n    import gensim\nexcept:\n    !pip install gensim\ntry:\n    import wordcloud\nexcept:\n    !pip install wordcloud\ntry:\n    import nltk\nexcept:\n    !pip install nltk\n!python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_md", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import spacy\n\nfrom collections import Counter, defaultdict,OrderedDict\n\nimport pandas as pd\nimport os\nimport csv\nimport itertools\nimport re\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport string\n\nfrom spacy_langdetect import LanguageDetector\nimport plac\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.tokens import Doc, Span, Token\n\nfrom langdetect import detect, detect_langs\nfrom nltk.tokenize import sent_tokenize\n\nimport nltk\nnltk.download('punkt')", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package punkt to /home/wsuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n", "name": "stderr"}, {"output_type": "execute_result", "execution_count": 1, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "apt_covid_notam_lt_df = pd.read_csv(\"/project_data/data_asset/all_airports_covid_notams_20200717.csv\")\nasp_covid_notam_lt_df = pd.read_csv(\"/project_data/data_asset/all_airspaces_covid_notams_20200717.csv\")", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(apt_covid_notam_lt_df.airportCode.unique())", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "1484"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "**1. Extract NOTAM from json**"}, {"metadata": {}, "cell_type": "code", "source": "def decode_airport_message(df):\n    apt_dt = dict()\n    apt_dt['message'] = []\n    apt_dt['Qcode'] = []\n    apt_dt['createdDate'] = []\n    apt_dt['Closed'] = []\n    apt_dt['airportName'] = []\n    apt_dt['airportCode'] = []\n    apt_dt['cityName'] = []\n    apt_dt['countryCode'] = []\n    apt_dt['countryName'] = []\n    apt_dt['latitude'] = []\n    apt_dt['longitude'] = []\n    \n    for idx,row in df.iterrows():\n        if type(row['notams'])==str:\n            jsd = json.loads(row['notams'])\n            for m in jsd['message'].values():\n                m_ = m.split(\"\\nCREATED: \")[0]\n                m_ = m_.replace('\\n',' ')\n                #print(m_)\n                apt_dt['message'].append(m_)\n                apt_dt['Closed'].append(row['Closed'])\n                apt_dt['airportName'].append(row['airportName'])\n                apt_dt['airportCode'].append(row['airportCode'])\n                apt_dt['cityName'].append(row['cityName'])\n                apt_dt['countryCode'].append(row['countryCode'])\n                apt_dt['countryName'].append(row['countryName'])\n                apt_dt['latitude'].append(row['latitude'])\n                apt_dt['longitude'].append(row['longitude'])\n\n            for qc in jsd['Qcode'].values():\n                apt_dt['Qcode'].append(qc)\n            for cd in jsd['Created'].values():\n                apt_dt['createdDate'].append(cd)\n        else:\n            apt_dt['message'].append(None)\n            apt_dt['Qcode'].append(None)\n            apt_dt['createdDate'].append(None)\n            apt_dt['Closed'].append(row['Closed'])\n            apt_dt['airportName'].append(row['airportName'])\n            apt_dt['airportCode'].append(row['airportCode'])\n            apt_dt['cityName'].append(row['cityName'])\n            apt_dt['countryCode'].append(row['countryCode'])\n            apt_dt['countryName'].append(row['countryName'])\n            apt_dt['latitude'].append(row['latitude'])\n            apt_dt['longitude'].append(row['longitude'])\n\n    apt_df = pd.DataFrame(apt_dt)\n    apt_df['createdDate'] = pd.to_datetime(apt_df['createdDate'])\n    return apt_df", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def decode_airspace_message(df):\n    apt_dt = dict()\n    apt_dt['message'] = []\n    apt_dt['Qcode'] = []\n    apt_dt['createdDate'] = []\n    apt_dt['Closed'] = []\n    apt_dt['FIRcode'] = []\n    apt_dt['FIRname'] = []\n    apt_dt['countryCode'] = []\n    apt_dt['countryName'] = []\n    \n    for idx,row in df.iterrows():\n        if type(row['notams'])==str:\n            jsd = json.loads(row['notams'])\n            for m in jsd['message'].values():\n                m_ = m.split(\"\\nCREATED: \")[0]\n                m_ = m_.replace('\\n',' ')\n                #print(m_)\n                apt_dt['message'].append(m_)\n                apt_dt['FIRname'].append(row['FIRname'])\n                apt_dt['FIRcode'].append(row['FIRcode'])\n                apt_dt['countryCode'].append(row['countryCode'])\n                apt_dt['countryName'].append(row['countryName'])\n\n            for qc in jsd['Qcode'].values():\n                apt_dt['Qcode'].append(qc)\n            for cd in jsd['Created'].values():\n                apt_dt['createdDate'].append(cd)\n            for c_ in jsd['Closed'].values():\n                apt_dt['Closed'].append(c_)\n        else:\n            apt_dt['message'].append(None)\n            apt_dt['Qcode'].append(None)\n            apt_dt['createdDate'].append(None)\n            apt_dt['Closed'].append(None)\n            apt_dt['FIRname'].append(row['FIRname'])\n            apt_dt['FIRcode'].append(row['FIRcode'])\n            apt_dt['countryCode'].append(row['countryCode'])\n            apt_dt['countryName'].append(row['countryName'])\n\n    apt_df = pd.DataFrame(apt_dt)\n    apt_df['createdDate'] = pd.to_datetime(apt_df['createdDate'])\n    return apt_df", "execution_count": 5, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "apt_df = decode_airport_message(apt_covid_notam_lt_df)\nasp_df = decode_airspace_message(asp_covid_notam_lt_df)", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**The following API gives a consolidated NOTAMS of Airport and Airspace Restrictions related to COVID 19**\n\nList of NOTAMS for airspaces and aairports referring to COVID-19 restrictions\n\nData dictionary of Airport COVID-19 NOTAMS\n\n|Field|\tType|\tDescription|\n|-----|-----|-----|\ncountryName|\tstring|\tName of the Country|\ncountryCode|\tstring|\tISO 3-Letter Code of the Country|\nairportName|\tstring|\tName of the airport, searchable|\ncityName|\tstring|\tName of the city, searchable|\nairportCode|\tstring|\tICAO 4-letter code of the airport|\nlatitude|\tnumber|\tLatitude in Decimal degrees|\nlongitude|\tnumber|\tLongitude in Decimal degrees|\nNoTraffic|\tstring|\tWheather the airport has less than one flight per day in the last 7 days (TRUEor FALSE)|\nClosed|\tstring|\tIf the airport has a NOTAM which is Q-code FALC (TRUE or FALSE), which means the airport is closed |\ntraffic|\tstring|\tTraffic data of the reference week, previous week and current week (json stringified format)|\nnotams|\tstring|\tNOTAMS containing COVID or CORONAVIRUS key words for the airport (json stringified format)|\nmessages| string| NOTAMS message as a string|\nQcode| string| Qcode of the NOTAM|\ncreatedDate| datetime| NOTAM created date|\n\nQcode reference: https://www.notams.faa.gov/common/qcode/qcode.html"}, {"metadata": {}, "cell_type": "markdown", "source": "**Preprocessing**"}, {"metadata": {}, "cell_type": "code", "source": "############\n#Stop words#\n############\n\nnlp_ = spacy.load('en_core_web_md')\n\n# Adding stop words\nnew_stop_words = [\"create\",\"source\",\"euecyiyn\",'etczyoyx','tel']\n\n# Add airport codes to stop words\nnew_stop_words.extend([ac.lower() for ac in list(apt_covid_notam_lt_df.airportCode.values)])\n\n\nfor new_word in new_stop_words:\n    nlp_.vocab[new_word].is_stop = True\n\n# Add language detector to pipeline\nnlp_.add_pipe(LanguageDetector(), name='language_detector', last=True)\n    \n\n#https://proairpilot.com/faa-notam.html\n#https://www.icao.int/NACC/Documents/Meetings/2014/ECARAIM/REF03-ICAOCodes.pdf\nmapping = {\"acc\": \"area control\", \"acft\": \"aircraft\", \"ad\": \"aerodrome\", \"aic\": \"aeronautical information circular\",\n           \"aip\": \"aeronautical information publication\", \"ais\": \"aeronautical information services\",\n           \"alt\": \"altitude\", \"altn\": \"alternate\", \"ap\": \"airport\", \"aro\": \"air traffic services reporting office\",\n           \"arr\": \"arrival\", \"atc\": \"air traffic control\", \"ats\": \"air traffic services\", \"attn\": \"attention\",\n           \"auth\": \"authorized\", \"avbl\": \"available\", \"bfr\": \"before\", \"cat\": \"category\", \"chg\": \"change\",\"civ\":\"civil\",\n           \"clsd\": \"closed\", \"cov\": \"cover\", \"cta\": \"control area\", \"ctc\": \"contact\", \"ctr\": \"control zone\",\n           \"dem.\": \"democratic\", \"dep\": \"depart\", \"emerg\": \"emergency\", \"enr\": \"en route\", \"exc\": \"except\",\n           \"fed.\": \"federation\", \"fir\": \"flight information region\", \"fis\": \"flight information service\",\n           \"flt\": \"flight\", \"flts\": \"flights\", \"flw\": \"follows\", \"fm\": \"from\", \"fpl\": \"filed flight plan\",\n           \"fri\": \"friday\", \"gen\": \"general\", \"hr\": \"hour\", \"intl\": \"international\", \"isl.\": \"islands\",\n           \"ldg\": \"landing\", \"mil\": \"military\", \"mon\": \"monday\", \"op\": \"operation\",\"ops\": \"operations\", \n           \"opr\": \"operating\",\"pax\": \"passenger\",\n           \"ppr\": \"prior permission required\", \"ref\": \"refernce to\", \"rep.\": \"republic\", \"req\": \"request\",\n           \"rffs\": \"rescue and fire fighting services\", \"rmk\": \"remark\", \"rte\": \"route\", \"rwy\": \"runway\",\n           \"sat\": \"saturday\", \"ser\": \"service\", \"svc\": \"service message\", \"taf\": \"terminal aerodrome forecast\",\n           \"tfc\": \"traffic\", \"thu\": \"thursday\", \"tma\": \"terminal control area\", \"tue\": \"tuesday\",\n           \"twr\": \"aerodrome control tower\", \"vfr\": \"visual flight rules\"}\n", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**2. Cleaning the NOTAMs**"}, {"metadata": {}, "cell_type": "code", "source": "def clean_message(message):\n    #############################\n    ## mapping, lower, language #\n    #############################\n    #Make everything to lower case\n    message = row['message'].lower().strip()\n    message = re.sub(r'(http|https|www)\\S+', '', message)\n    \n    # Mapping short terms to actual word\n    for s_, w in mapping.items():\n        message = re.sub(r'\\b{}\\b'.format(s_),w,message)\n    \n    # If a country has NOTAMs in two languages, the english text starts with the phrase \"english text/english version\"\n    if \"english text\" in message:\n        #print(\"TEXT\")\n        e_t = []\n        for m_ in message.split(\"english text\"):\n            for sent_ in sent_tokenize(m_):\n                if detect(sent_) == \"en\":\n                    e_t.append(sent_)\n        message = \"\".join(e_t)\n\n    elif \"english version\" in message:\n        #print(\"VERSION\")\n        e_t = []\n        for m_ in message.split(\"english version\"):\n            for sent_ in sent_tokenize(m_):\n                if detect(sent_) == \"en\":\n                    e_t.append(sent_)\n        \n        message = \"\".join(e_t)\n        #message = message.split(\"english version\")[1]\n    elif detect(message) != \"en\":\n        message = \"\"\n\n    #Start of string other than character or digit\n    message = re.sub(r'[^ 0-9a-z]', ' ', message)\n    message = message.translate(message.maketrans('', '', string.punctuation)) #extra punctuations removal\n\n    # Remove unnecessary white space\n    message = \" \".join(message.split())\n    \n    return message", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**3. Generating tokens**"}, {"metadata": {}, "cell_type": "code", "source": "def generate_tokens(message):\n    sent_ = nlp_(message)\n    # Cleaning text\n    tokens = []\n    for token in sent_:\n        # Remove punctuation and numbers\n        # Get only date digits!\n        if token.is_alpha:\n            # Lemma\n            lemma_text = token.lemma_\n            #Remove stop words\n            if not nlp_.vocab[lemma_text].is_stop:\n                if len(lemma_text) > 2:\n                    tokens.append(lemma_text)\n    return tokens", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Airport COVID-19 NOTAMS**"}, {"metadata": {}, "cell_type": "code", "source": "#words = set(nltk.corpus.words.words())\napt_df['tokens'] = None\napt_df['cleaned_message'] = None\n\nfor idx,row in apt_df.iterrows():\n    if row['message'] is not None:\n        message = row['message']\n        \n        message_ = clean_message(message)\n        tokens = generate_tokens(message_)\n        \n        apt_df.at[idx,\"cleaned_message\"] = message_\n        apt_df.at[idx,\"tokens\"] = tokens", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Airspace COVID-19 NOTAMS**"}, {"metadata": {}, "cell_type": "code", "source": "#words = set(nltk.corpus.words.words())\nasp_df['tokens'] = None\nasp_df['cleaned_message'] = None\n\nfor idx,row in asp_df.iterrows():\n    if row['message'] is not None:\n        message = row['message']\n        \n        message_ = clean_message(message)\n        tokens = generate_tokens(message_)\n        \n        asp_df.at[idx,\"cleaned_message\"] = message_\n        asp_df.at[idx,\"tokens\"] = tokens", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "valid_apt_df = apt_df.dropna(subset=['message'])\nvalid_apt_df = valid_apt_df[valid_apt_df.cleaned_message != '']\n\nvalid_asp_df = asp_df.dropna(subset=['message'])\nvalid_asp_df = valid_asp_df[valid_asp_df.cleaned_message != '']", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Unresolved abbreviations**\n\nde, del,atr,sar (this refes to special administrative zone as well as search and rescue),\ndgac (some authority center),caa(some authority center),enac(some authority center),hum,act (multiple meanings)"}, {"metadata": {}, "cell_type": "markdown", "source": "**Qcode**\n\nQcode is a brevity code that informs the type of message being sent. For our analysis we can ignore some Qcodes such as aerodrome service hours as we are mainly looking for quarantine duration as well as country restrictions"}, {"metadata": {}, "cell_type": "markdown", "source": "\n|Qcode| Meaning|\n|----|-----|\nFAXX | Aerodrome - other|\nFAAH | Aerodrome - HOURS OF SERVICE ARE|\nFALT | Aerodrome - LIMITED TO|\nFFCG | FIRE FIGHTING AND RESCUE - DOWNGRADED TO|\nXXXX | Other - Other|\nFALC | Aerodrome - CLOSED|\nACXX | CLASS B, C, D OR E SURFACE AREA (ICAO-CONTROL ZONE) - OTHER|\nFAAP | Aerodrome - PRIOR PERMISSION REQUIRED|\nSPAH | APPROACH CONTROL - HOURS OF SERVICE ARE|\nAFXX |FLIGHT INFORMATION REGION (FIR) - OTHER|\nOEXX | AIRCRAFT ENTRY REQUIREMENTS - OTHER| \nOECA |AIRCRAFT ENTRY REQUIREMENTS - \nOAXX |AERONAUTICAL INFORMATION SERVICE - OTHER\nSEAH |FLIGHT INFORMATION SERVICE -HOURS OF SERVICE ARE "}, {"metadata": {}, "cell_type": "markdown", "source": "Qcode to be excluded: FAAH, FFCG, SPAH other codes that end with AH\n    \nQcode of interest: FAXX, FALC, FALT, OEXX, OECA\n    \nQcode not sure: ACXX, FAAP"}, {"metadata": {}, "cell_type": "code", "source": "valid_asp_df.to_csv(\"/project_data/data_asset/ws2/notams/valid_airspace_notams_20200717.csv\", index=False,quoting=csv.QUOTE_NONNUMERIC)\nvalid_apt_df.to_csv(\"/project_data/data_asset/ws2/notams/valid_airport_notams_20200717.csv\",index=False,quoting=csv.QUOTE_NONNUMERIC)", "execution_count": 15, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "markdown", "source": "**Author**\n\n* Shri Nishanth Rajendran - AI Development Specialist, R\u00b2 Data Labs, Rolls Royce"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}