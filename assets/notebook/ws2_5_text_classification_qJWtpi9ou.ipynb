{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Text Classification"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook includes the process of training a binary classification model on textual data. Please note that the notebook runs on a small sample dataset, and the generated results are not usable.\n\n#### Input:\n\nws_2_article_topic_XX.csv:\nThis dataset contains the clean text and the LDA results as features obtained from `ws2_2_topic_modelling` notebook where XX is the optimal number of topics. We create a binary label using 'topic_label' of this dataset.\n\n#### Output:\n\n'covid_classifier' model:\nThis notebook trains a binary classification model using SGDClassifier of sklearn and deploy it in the IBM Cloud Pak for Data deployment space.\n\ntf_idf.csv:\nThe code will produce the tf_idf matrix as CSV for training the classification model with AutoAI.\n\n\n#### Classification workflow includes:\n\n- Import data\n- Data split and upsampling\n- Pipeline of TF-IDF tokenization method and Linear SVM classifier using SGDClassifier\n- Hyper-parameter tuning with grid search\n- Model evaluation\n- Prepare data for AutoAI\n- Save and deploy model with WML"}, {"metadata": {}, "cell_type": "code", "source": "# Importing Libraries\n\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics, ensemble\n\nimport warnings\nwarnings.filterwarnings('ignore')", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Configuration parameters\n\nThe data path of the data storage"}, {"metadata": {}, "cell_type": "code", "source": "# The path to the output folder where all the outputs will be saved\noutput_path = \"/project_data/data_asset\"", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Import data\n\nData contains clean articles and topic labels that are generated based on the result of LDA analysis with measuring the coherence metric. Here we import data and create a binary label where `topic_of_interest`is the topic we aim to detect with a classification model."}, {"metadata": {}, "cell_type": "code", "source": "# Importing Article Dataset\n\ndf_clean = pd.read_csv(f\"{output_path}/ws_2_article_topic_6.csv\")\ndf_clean.head()", "execution_count": 3, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>article</th>\n      <th>date</th>\n      <th>n_words</th>\n      <th>article_clean</th>\n      <th>n_words_clean</th>\n      <th>dominant_topic</th>\n      <th>weight</th>\n      <th>keywords</th>\n      <th>topic_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>an acquaintance in Post-Polio &gt;Syndrome This a...</td>\n      <td>2020-01-01</td>\n      <td>160</td>\n      <td>acquaintance post polio syndrome apparently ne...</td>\n      <td>64</td>\n      <td>5</td>\n      <td>0.5039</td>\n      <td>['health, disease, medical, patient, year, inf...</td>\n      <td>label_6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>homore in high school. Before that, I used to ...</td>\n      <td>2020-01-02</td>\n      <td>81</td>\n      <td>homore high school bloody nose nighttime asthm...</td>\n      <td>26</td>\n      <td>0</td>\n      <td>0.4987</td>\n      <td>['science, problem, well, pain, much, thing, d...</td>\n      <td>label_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>7:26 GMT On 5 Apr 93 23:27:26 GMT, (Vida Morku...</td>\n      <td>2020-01-03</td>\n      <td>280</td>\n      <td>gmt apr gmt vida morkunas vida inner ear probl...</td>\n      <td>123</td>\n      <td>0</td>\n      <td>0.9729</td>\n      <td>['science, problem, well, pain, much, thing, d...</td>\n      <td>label_1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>tes about heat shock proteins (HSP's) and DNA....</td>\n      <td>2020-01-04</td>\n      <td>159</td>\n      <td>te heat shock protein hsp dna hate derogatory ...</td>\n      <td>55</td>\n      <td>0</td>\n      <td>0.5005</td>\n      <td>['science, problem, well, pain, much, thing, d...</td>\n      <td>label_1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>now mapping the human genome. We will &gt; then s...</td>\n      <td>2020-01-05</td>\n      <td>290</td>\n      <td>human genome manipulation genome genetic engin...</td>\n      <td>119</td>\n      <td>1</td>\n      <td>0.2706</td>\n      <td>['study, food, msg, people, blood, human, arti...</td>\n      <td>label_2</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   article_id                                            article        date  \\\n0           0  an acquaintance in Post-Polio >Syndrome This a...  2020-01-01   \n1           1  homore in high school. Before that, I used to ...  2020-01-02   \n2           2  7:26 GMT On 5 Apr 93 23:27:26 GMT, (Vida Morku...  2020-01-03   \n3           3  tes about heat shock proteins (HSP's) and DNA....  2020-01-04   \n4           4  now mapping the human genome. We will > then s...  2020-01-05   \n\n   n_words                                      article_clean  n_words_clean  \\\n0      160  acquaintance post polio syndrome apparently ne...             64   \n1       81  homore high school bloody nose nighttime asthm...             26   \n2      280  gmt apr gmt vida morkunas vida inner ear probl...            123   \n3      159  te heat shock protein hsp dna hate derogatory ...             55   \n4      290  human genome manipulation genome genetic engin...            119   \n\n   dominant_topic  weight                                           keywords  \\\n0               5  0.5039  ['health, disease, medical, patient, year, inf...   \n1               0  0.4987  ['science, problem, well, pain, much, thing, d...   \n2               0  0.9729  ['science, problem, well, pain, much, thing, d...   \n3               0  0.5005  ['science, problem, well, pain, much, thing, d...   \n4               1  0.2706  ['study, food, msg, people, blood, human, arti...   \n\n  topic_label  \n0     label_6  \n1     label_1  \n2     label_1  \n3     label_1  \n4     label_2  "}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "code", "source": "# Set the topic of interest for bianry classification\ntopic_of_interest = 'label_5'\n\n# create a binary label where 1 is the topic of interest and 0 is the rest\ndf_clean['label'] = np.where(df_clean['topic_label'] == topic_of_interest, 1, 0)  \ndf_clean['label'].value_counts()", "execution_count": 4, "outputs": [{"data": {"text/plain": "0    125\n1     12\nName: label, dtype: int64"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Data split and upsampling\n\nThe dataset is inbalance in the label column, significantly more 0 class than 1 class. Upsampling will allow us increase the proportion of 1 class in data."}, {"metadata": {}, "cell_type": "code", "source": "# Spliting the data into train and test sets\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# cleaned body of documents\nX = df_clean['article_clean']  \n# Target variable\ny = df_clean['label'] \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    stratify=y, \n                                                    test_size=0.25,\n                                                    random_state = 0)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Upsampling\nfrom sklearn.utils import resample\n\n# concatenate our training data back together\ntrain_concat = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nmajority_class = train_concat[train_concat.label==0]\nminority_class = train_concat[train_concat.label==1]\n\n# upsample the minority class\nminority_upsampled = resample(minority_class,\n                              replace=True, # sample with replacement\n                              n_samples=len(majority_class), # match number in majority class\n                              random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([majority_class, minority_upsampled])\n\n#shuffle \nupsampled = upsampled.sample(frac=1).reset_index(drop=True)\n\ny_train = upsampled['label']\nX_train = upsampled['article_clean']", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Pipeline\n\nCreation of the step by step process that will lead to the classification"}, {"metadata": {}, "cell_type": "code", "source": "# Create the pipeline\npipeline = Pipeline([\n    ('vect',TfidfVectorizer(max_df = 0.8, min_df=0.0001, norm = 'l2', use_idf = True)), \n    ('clf',SGDClassifier(random_state=0, alpha = 2e-05, penalty = 'l2', loss = 'hinge')) \n])", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Hyper-parameter tuning with grid search\n\nFinding optimal parameters for the classifier"}, {"metadata": {}, "cell_type": "code", "source": "# Define parameters\nparameters = {\n    'vect__max_df': (0.6, 0.7, 0.8), # max threshold for document frequency\n    'vect__min_df': (0.0001, 0.001),\n    'vect__max_features': (None, 5000, 10000), # top max_features ordered by term frequency across the corpus\n    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    'vect__use_idf': (True, False), # Enables inverse-document-frequency reweighting\n    'vect__norm': (None, 'l1', 'l2'), # normalizes tf-idf in each row\n    'clf__loss': ('log','modified_huber','hinge'), # log for logistic regression, modified_huber gives a smooth loss tolerant to outliers, hinge for linear SVM\n    'clf__alpha': (0.00001,0.00002), # multiplies the regularization term\n    'clf__penalty': ('none','l2','l1'), # regularization term\n    'clf__max_iter': (10, 50), # passes over the training data (aka epochs)\n}", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Grid search with cross validation\ngrid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='roc_auc', n_jobs=10, verbose=1, iid=False) # 'roc_auc' or 'f1'\n\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\n\n# model training\ngrid_search.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "### Result of grid search\nprint(\"Best score: %0.3f\" % grid_search.best_score_)  \nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "execution_count": 10, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best score: 1.000\nBest parameters set:\n\tclf__alpha: 1e-05\n\tclf__loss: 'log'\n\tclf__max_iter: 10\n\tclf__penalty: 'none'\n\tvect__max_df: 0.6\n\tvect__max_features: None\n\tvect__min_df: 0.0001\n\tvect__ngram_range: (1, 1)\n\tvect__norm: 'l1'\n\tvect__use_idf: True\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Scoring the test set\n\nSelecting best model from grid search and predictions"}, {"metadata": {}, "cell_type": "code", "source": "# best estimator found from grid search\nmodel = grid_search.best_estimator_", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Model predictions\n\npredictions = model.predict(X_test)\npredictions", "execution_count": 12, "outputs": [{"data": {"text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Model evaluation\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# Return of the Accuracy, Precision, Recall, F1 Score, AUC score\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))\nprint(\"Precision:\",metrics.precision_score(y_test, predictions))\nprint(\"Recall:\",metrics.recall_score(y_test, predictions))\nprint(\"f1_score:\",metrics.f1_score(y_test, predictions))\nprint('AUC: ', metrics.roc_auc_score(y_test, predictions))", "execution_count": 13, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Accuracy: 0.9142857142857143\nPrecision: 0.0\nRecall: 0.0\nf1_score: 0.0\nAUC:  0.5\n"}, {"name": "stderr", "output_type": "stream", "text": "/opt/conda/envs/Python-3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/envs/Python-3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"}]}, {"metadata": {}, "cell_type": "code", "source": "# confusion matrix of predictions\n\ncnf_matrix = metrics.confusion_matrix(y_test, predictions)\ncnf_matrix", "execution_count": 14, "outputs": [{"data": {"text/plain": "array([[32,  0],\n       [ 3,  0]])"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Prepare data for AutoAI\n\nextracting TF-IDF matrix with 1000 most important words with label to input into AutoAI"}, {"metadata": {}, "cell_type": "code", "source": "# create tf-idf matrix\nTvectorizer = TfidfVectorizer(max_df = 0.6, min_df=0.0001, norm = 'l2', max_features=1000)\nX_tfidf = Tvectorizer.fit_transform(df_clean['article_clean'])\n# place tf-idf values in a pandas data frame\ntfidf_df = pd.DataFrame(X_tfidf.todense(), columns=Tvectorizer.get_feature_names())", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# add label column to dataframe\ntfidf_df['label'] = df_clean['label']", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# save data\ntfidf_df.to_csv(f\"{output_path}/tf_idf.csv\")", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Save and deploy model with WML\n\nWe save and deploy the model by connecting to the ICP4D local Watson Machine Learning using CP4D credentials. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their models. We can save models, functions and data assets in this space. The steps involved in saving and deploying the model are detailed in the following cells. We will use the watson_machine_learning_client package to complete these steps.\n\n* Connect to WML client\n* Save the model in the deployment space repository\n* Deploy the model ONLINE\n\n### Connect to WML client\n\nWe will use the watson_machine_learning_client package to complete these steps. We establish a connection to the Watson Machine Learning API with the system credentials and set the default space and project."}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\nimport os\n\ntoken = os.environ['USER_ACCESS_TOKEN']\n\nwml_credentials = {\n   \"token\": token,\n   \"instance_id\" : \"openshift\",\n   \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n   \"version\": \"2.5.0\"\n}\n\nclient = WatsonMachineLearningAPIClient(wml_credentials)", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create a deployment space (only run for the first time)\nspace_details = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: \"text_analysis_space\"}) \n\n# get the space uid\nspace_uid = client.spaces.get_uid(space_details)", "execution_count": 28, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set project ID\nproject_uid = os.environ['PROJECT_ID']\nclient.set.default_project(project_uid)\nclient.set.default_space(space_uid)", "execution_count": 29, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Unsetting the project_id ...\n"}, {"data": {"text/plain": "'SUCCESS'"}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Save the model in the deployment space repository"}, {"metadata": {}, "cell_type": "code", "source": "# give model name\nmodel = model\nmodel_name = 'covid_classifier'", "execution_count": 30, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Store the model details\nmodel_props = {client.repository.ModelMetaNames.NAME: model_name,\n               client.repository.ModelMetaNames.RUNTIME_UID : \"scikit-learn_0.20-py3.6\",\n               client.repository.ModelMetaNames.TYPE : \"scikit-learn_0.20\",\n               }\n\n# store model in the deployment space\nstored_model_details = client.repository.store_model(model=model, meta_props=model_props)", "execution_count": 31, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Deploy the model ONLINE"}, {"metadata": {}, "cell_type": "code", "source": "# deployment metadata of the model\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: model_name,\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# deploy the model\nmodel_uid = stored_model_details[\"metadata\"][\"guid\"]\ndeployment_details = client.deployments.create( artifact_uid=model_uid, meta_props=meta_props)", "execution_count": 32, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '0cf6eced-bd40-45b2-8357-92c70c26eb04' started\n\n#######################################################################################\n\n\ninitializing\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='d3e02ca1-fe6e-4572-89f7-8c3d002f1ef6'\n------------------------------------------------------------------------------------------------\n\n\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Score the deployed model"}, {"metadata": {}, "cell_type": "code", "source": "fields = [\"article_clean\"]\nvalues = [['virus hero key worker full list classify niamh cavanagh mar update mar boris johnson announce today school shut friday notice child key worker vulnerable kid official list release tomorrow list medical professional education name johnson announcement earlier today credit afp list medical professional include doctor nurse midwife paramedic health worker employment medical health community safety police force fire brigade education teacher worker school pre school supermarket worker delivery driver broadcast johnson announce school whole close friday britain desperately contain coronavirus outbreak right time education secretary gavin williamson promise free school meal everyone likely form supermarket voucher boris nation even slow result draconian measure place week believe step already together announce today already slow spread disease pay story']]\n\nscoring_payload = {\nclient.deployments.ScoringMetaNames.INPUT_DATA: [{\n    \"fields\": fields, \n    \"values\": values\n}]\n}", "execution_count": 33, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dep_id = client.deployments.get_uid(deployment_details)", "execution_count": 34, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.deployments.score(deployment_id=dep_id,meta_props=scoring_payload)", "execution_count": 35, "outputs": [{"data": {"text/plain": "{'predictions': [{'fields': ['prediction', 'probability'],\n   'values': [[0, [0.8675093568284711, 0.13249064317152887]]]}]}"}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Authors\n* **Mehrnoosh Vahdat** is Data Scientist with Data Science & AI Elite team where she specializes in Data Science, Analytics platforms, and Machine Learning solutions.\n* **Anthony Ayanwale** is Data Scientist with CPAT team where he specializes in Data Science, Analytics platforms, and Machine Learning solutions."}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}