{"cells": [{"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\ntry:\n    import filterpy\n    import geopy\nexcept ImportError:\n    !pip install filterpy\n    !pip install geopy\nfrom filterpy.kalman import KalmanFilter, EnsembleKalmanFilter, UnscentedKalmanFilter, MerweScaledSigmaPoints\nfrom filterpy.common import Q_discrete_white_noise, Saver\nimport loggings\nlogging.getLogger('matplotlib').setLevel(logging.WARNING)\nplt.rcParams.update({'figure.max_open_warning': 0})\n\npd.set_option(\"display.max_columns\", 500)\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n", "execution_count": 2, "outputs": [{"output_type": "error", "ename": "ModuleNotFoundError", "evalue": "No module named 'loggings'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-ac2869cb653e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfilterpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkalman\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKalmanFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnsembleKalmanFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnscentedKalmanFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerweScaledSigmaPoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfilterpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQ_discrete_white_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSaver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mloggings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'figure.max_open_warning'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'loggings'"]}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# data = pd.read_csv('https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "path = \"https://raw.githubusercontent.com/rs-delve/covid19_datasets/master/dataset/combined_dataset_latest.csv\"\n\nstringency = pd.read_csv(path, parse_dates = ['DATE'])\n\n# stringency['country_name'] = stringency['country_name'].apply(lambda z : z.upper()) \n\nstringency.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stringency[stringency['ISO']=='BEL']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Test condition"}, {"metadata": {}, "cell_type": "code", "source": "# region name or None for country-level aggregation\nregion = None # 'Flanders', 'Brussels', 'Wallonia' or None\n# date range to be used (must match with '%m-%d' format)\ndate_range = None\n# date_range = ['03-15', '05-09'] # if date_range is None, use all dates\n# interpolate cases and tests values during the weekends?\ninterpolate_weekend = False", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Load data"}, {"metadata": {}, "cell_type": "code", "source": "data_file = 'https://epistat.sciensano.be/Data/COVID19BE.xlsx'\n#data_file = 'data/Belgium/COVID19BE.xlsx'\nxls = pd.ExcelFile(data_file)\ndf_hosp = pd.read_excel(xls, 'HOSP')\ndf_hosp.columns = map(str.lower, df_hosp.columns)\nif region:\n    df_hosp = df_hosp[df_hosp.region == region]\ndf_hosp.date = pd.to_datetime(df_hosp.date, format='%Y-%m-%d').dt.strftime('%Y-%m-%d')\ndf_hosp = df_hosp.sort_values('date').set_index('date')\ndf_hosp = df_hosp.groupby('date').sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_hosp.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Select dates of interest"}, {"metadata": {}, "cell_type": "code", "source": "# default is without year. if year is required, make sure to modify accordingly.\nif date_range:\n    if date_range[0] is None:\n        date_range[0] = '00-00'\n    if date_range[1] is None:\n        date_range[1] = '99-99'\n    df_hosp = df_hosp[(df_hosp.index >= date_range[0]) & (df_hosp.index <= date_range[1])]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_hosp.index.max()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Interpolate weekends if needed"}, {"metadata": {}, "cell_type": "code", "source": "def interpolate_weekend(x, col, first_friday):\n    print('Interpolate for %s (Fri, Sat, Sun, Mon)' % col)\n    first_friday_index = np.flatnonzero((x.index==first_friday))[0]\n    for i in range(first_friday_index, x.shape[0]-3, 7):\n        delta = (x.loc[x.index[i+3], col] - x.loc[x.index[i], col]) / 3\n        value_sat = x.loc[x.index[i], col] + delta\n        value_sun = value_sat + delta\n#         print('[%s:%s] %7.1f, %7.1f > %-7.1f, %7.1f > %-7.1f, %7.1f' % (x.index[i], x.index[i+3],\\\n#             x.loc[x.index[i], col], x.loc[x.index[i+1], col], value_sat, x.loc[x.index[i+2], col], value_sun, x.loc[x.index[i+3], col]))\n        x.loc[x.index[i+1], col] = value_sat\n        x.loc[x.index[i+2], col] = value_sun\n        return x\n\nif interpolate_weekend is True:\n    first_friday = '0320'\n    interpolate_weekend(df_case, 'cases', first_friday)\n    interpolate_weekend(df_case, 'vul_count', first_friday)\n    interpolate_weekend(df_test, 'tests', first_friday)\n\n    # recalculate vul_ratio after interpolation\n    df_case['vul_ratio'] = df_case.vul_count / df_case.cases", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Kalman filter models"}, {"metadata": {}, "cell_type": "code", "source": "def kalman_predictor(initial_state, kf_p, kf_r, kf_q, kf_a):\n    \"\"\"\n    We model Covid development as a dynamical system composed of 3 components:\n    - measurement (observable) = case count,\n    - speed (latent) = growth rate (cases per day)\n    - acceleration (latent) = growth acceleration (cases per day^2)    \n    - used params of kf_p=0, kf_r=10, kf_q=20\n    \"\"\"\n    # day is our observation interval\n    dt = 1\n    # transition matrix (x:measurement, v:growth rate, a:growth acceleration)\n    F = np.array([[1, dt, 0.5*(dt**2)], # x_new           = x_old + v*dt + 1/2*a*dt^2\n                    [0, 1, dt],           # d(v_new) / dt   = v     + a*dt + 0\n                    [0, 0, 1]])           # d(x_new) / dt^2 = 0     + 0    + a\n    '''\n    F = np.array([[1, dt, (dt**2)/2, (dt**3)/6], # x_new = x_old + v*dt + 1/2*a*dt^2 + 1/6*j*dt^3\n                  [0, 1, dt, (dt**2)/2],         # d(x_new) / dt = v + a*dt + 1/2*j*dt^2\n                  [0, 0, 1, dt],                 # d(v_new) / dt = a + jt\n                  [0, 0, 0, 1]])                 # d(a_new) / dt = j\n    '''\n    # define a linear KF with position, velocity, acceleration parameters\n    dim_x = F.shape[0]\n    kf = KalmanFilter(dim_x=dim_x, dim_z=1)\n    kf.F = F\n    # state vector: initial position, velocity, acceleration\n    kf.x = np.zeros(dim_x)\n    kf.x[0] = initial_state\n    # measuremnet matrix: can only directly measure case counts, not velocity & acceleration\n    kf.H = np.zeros((1, dim_x))\n    kf.H[0][0] = 1\n    # covariance matrix\n    kf.P *= kf_p\n    # measurement noise\n    kf.R = kf_r\n    # process noise\n    kf.Q = Q_discrete_white_noise(dim=dim_x, dt=1, var=kf_q)\n    # fading factor\n    kf.alpha = kf_a\n    return kf\n\ndef ensemble_kalman_predictor(initial_state, kf_p, kf_r, kf_q, kf_n):\n    \"\"\"\n    We model Covid development as a dynamical system composed of 3 components:\n    - measurement (observable) = case count,\n    - speed (latent) = growth rate (cases per day)\n    - acceleration (latent) = growth acceleration (cases per day^2)\n    \"\"\"\n    # day is our observation interval\n    dt = 1\n    # transition matrix (x:measurement, v:growth rate, a:growth acceleration)\n    F = np.array([[1, dt, (dt**2)/2], # x_new           = x_old + v*dt + 1/2*a*dt^2\n                  [0, 1, dt],           # d(x_new) / dt   = v     + a*dt + 0\n                  [0, 0, 1]])           # d(x_new) / dt^2 = 0     + 0    + a\n    ''' also consider jerk\n    F = np.array([[1, dt, (dt**2)/2, (dt**3)/6], # x_new = x_old + v*dt + 1/2*a*dt^2 + 1/6*j*dt^3\n                  [0, 1, dt, (dt**2)/2],         # d(x_new) / dt = v + a*dt + 1/2*j*dt^2\n                  [0, 0, 1, dt],                 # d(v_new) / dt = a + jt\n                  [0, 0, 0, 1]])                 # d(a_new) / dt = j\n    '''\n    # state vector: initial position, velocity, acceleration\n    X0 = np.zeros(F.shape[0])\n    X0[0] = initial_state\n    # transition function\n    Fx = lambda x, dt: np.dot(F, x)\n    # measuremnet function\n    Hx = lambda x: np.array([x[0]])\n    # covariance matrix\n    P = np.eye(F.shape[0]) * kf_p\n    # measurement noise\n    R = kf_r\n    # process noise\n    Q = Q_discrete_white_noise(dim=F.shape[0], dt=1, var=kf_q)\n    # let's make it\n    kf = EnsembleKalmanFilter(x=X0, P=P, dim_z=1, dt=1, N=kf_n, hx=Hx, fx=Fx)\n    kf.R = R\n    kf.Q = Q\n    return kf", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Forecast using Kalman filter"}, {"metadata": {}, "cell_type": "code", "source": "def kalman_forecast(series, days, kf_type, params):\n    \"\"\"\n    Forecast based on history data.\n    \n    Input\n    -----\n    series: Pandas Series object with dates being index in ascending order.\n    days: Prediction window length\n    kf_type: linear, unscented, ensemble\n    kf_*: Parameters for Kalman filter. Default values work reasonably well on several countries.\n    \n    Output\n    ------\n    Pandas DataFrame object with the following columns\n    pred_raw: Raw prediction\n    pred: Final prediction (with smoothing, etc.)\n    ci_*: Lower and upper bounds of CI\n    \"\"\"\n    if days <= 0:\n        raise ValueError\n    dates = series.index\n    if kf_type == 'linear':\n        if params is None:\n            params = {'kf_p':1, 'kf_r':4, 'kf_q':0.1, 'kf_a':1}\n        if params['kf_a'] < 1:\n            raise ValueError\n        kf = kalman_predictor(series[dates[0]], params['kf_p'], params['kf_r'], params['kf_q'], params['kf_a'])\n    elif kf_type == 'ensemble':\n        if params is None:\n            params = {'kf_p':100, 'kf_r':1000, 'kf_q':0.1, 'kf_n':1000}\n        kf = ensemble_kalman_predictor(series[dates[0]], params['kf_p'], params['kf_r'], params['kf_q'], params['kf_n'])\n    else:\n        raise NotImplementedError\n    \n    # fit model\n    for measurement in series:\n        kf.predict()\n        kf.update([measurement])\n    \n    # start forecasting, starting from the last observation date\n    \n    if isinstance(dates[-1], str):\n        \n         last_date = dt.datetime.strptime(dates[-1], '%Y-%m-%d')\n            \n    else:\n        \n        last_date = dates[-1]\n        \n    predictions = []\n    pred_acc = []\n    pred_vel = []\n    pred_dates = []\n    ci_bounds = []\n    for day in range(days):\n        future_date = (last_date + dt.timedelta(days=day+1))\n        pred_dates.append(future_date)\n        kf.predict()\n        predictions.append(kf.x[0])\n        pred_acc.append(kf.x[2])\n        pred_vel.append(kf.x[1])\n        ci_bounds.append(kf_ci_bound(kf))\n    \n    # smoothen and add confidence intervals\n    predictions = np.array(predictions)\n    predictions[np.where(predictions < 0)[0]] = 0\n    #smooth_buffer = list(series[dates[len(dates)-days+1:]])\n    #predictions_smooth = smoother(smooth_buffer + predictions, days)[days-1:]\n    predictions_smooth = smoother(predictions, days)\n    ci_bounds = np.array(ci_bounds)\n    ci_upper = predictions_smooth + ci_bounds\n    ci_lower = predictions_smooth - ci_bounds\n    ci_lower[np.where(ci_lower < 0)[0]] = 0\n    \n    df_pred = pd.DataFrame({'pred_raw':predictions, 'pred':predictions_smooth, 'pred_vel': pred_vel, \n                          'pred_acc': pred_acc, 'ci_lower':ci_lower, 'ci_upper':ci_upper}, index=pred_dates)\n    return df_pred\n\ndef kf_ci_bound(kf):\n    \"\"\"\n    Compute 95% confidence interval from KF's positive semi-definite covariance matrix\n    \n    returns a positive single-sided boundary (half) of the interval\n    -> CI = kf.x[0] +- kf_ci_bound(kf)\n    \"\"\"\n    return 1.96 * (np.diag(kf.P)[0])**0.5\n\ndef smoother(x, winsize, method = 'slide'):\n    if method == 'slide':\n        x_smooth = []\n        for i in range(len(x)):\n            x_smooth.append(np.mean(x[max(0, i-winsize+1):i+1]))\n    elif method == 'slide_recurse':\n        x_smooth = predictions.copy()\n        for i in range(len(x)):\n            x_smooth[i] = np.mean(x_smooth[max(0, i-winsize):i+1])\n    else:\n        raise NotImplementedError\n    assert(len(x) == len(x_smooth))\n    return np.array(x_smooth)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Helpers"}, {"metadata": {}, "cell_type": "code", "source": "def get_stats(observations, predictions):\n    r2 = r2_score(observations, predictions)\n    mae = mean_absolute_error(observations, predictions)\n    rmse = mean_squared_error(observations, predictions) ** 0.5\n    return r2, mae, rmse\n\ndef rescale(df_x):\n    x = df_x.copy()\n    x -= x.min()\n    x /= x.max()\n    return x", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Performance testing and debugging"}, {"metadata": {}, "cell_type": "code", "source": "def kalman_test(series, winsize, kf_type, params=None):\n    \"\"\"\n    To test the performance compared with true values along all data points\n    \n    Input\n    -----\n    series: Pandas Series object with dates being index in ascending order.\n    winsize: Prediction window size in number of days.\n    kf_type: linear, unscented, ensemble\n    kf_*: factors for Kalman filter. Default values work reasonably well.\n          For long-term prediction, usually increasing the fading factor (kf_a) helps.\n    \n    Output\n    ------\n    Pandas DataFrame object with the following columns\n    pred_raw: Raw prediction\n    pred: Final prediction (with smoothing, etc.)\n    obs: Ground-truth values\n    history: recursive prediction history at each time point (for debugging purpose)\n    \"\"\"\n    if winsize <= 0:\n        raise ValueError\n    observations = []\n    predictions = []\n    predictions_acc = []\n    predictions_vel = []\n    pred_dates = []\n    history = []\n    ci_bounds = []\n    dates = series.index.to_numpy()\n    if kf_type == 'linear':\n        if params is None:\n            params = {'kf_p':1, 'kf_r':4, 'kf_q':0.1, 'kf_a':1}\n        if params['kf_a'] < 1:\n            raise ValueError\n        kf = kalman_predictor(series[dates[0]], kf_type, params['kf_p'], params['kf_r'], params['kf_q'], params['kf_a'])\n    elif kf_type == 'ensemble':\n        if params is None:\n            params = {'kf_p':100, 'kf_r':1000, 'kf_q':0.1, 'kf_n':1000}\n        kf = ensemble_kalman_predictor(series[dates[0]], params['kf_p'], params['kf_r'], params['kf_q'], params['kf_n'])\n    else:\n        raise NotImplementedError(kf_type)\n    \n    for i in range(dates.shape[0]-winsize):\n        # save the current state of the model\n        saver = Saver(kf, skip_callable=True, save_current=True)\n        \n        # recursive prediction\n        history_window = [kf.x[0]]\n        date_window = [dates[i]]\n        for day in range(winsize):\n            kf.predict()\n            history_window.append(kf.x[0])\n            date_window.append(dates[i+day+1])\n        history.append(pd.DataFrame({'pred':history_window}, index=date_window))\n        pred_date = dates[i+winsize]\n        pred_dates.append(pred_date)\n        prediction = kf.x[0]\n        predictions.append(prediction)\n        predictions_vel.append(kf.x[1])\n        predictions_acc.append(kf.x[2])\n        observation = series[pred_date]\n        observations.append(observation)\n        ci_bounds.append(kf_ci_bound(kf))\n        \n        # restore model states and update to next day\n        for attr in saver.keys:\n            try:\n                setattr(kf, attr, getattr(saver, attr)[-1])\n            except AttributeError: # property decoration causes problem\n                #print('.%s skip' % attr)\n                continue\n        kf.predict() ## is this predict call necessary at all?\n        kf.update([series[dates[i+1]]])\n    \n    # smoothen output\n    predictions_smooth = smoother(predictions, winsize)\n    predictions_smooth[predictions_smooth < 0] = 0\n    #acc_smooth = smoother(predictions_acc, winsize)\n    #vel_smooth = smoother(predictions_vel, winsize)\n    acc_smooth = predictions_acc\n    vel_smooth = predictions_vel\n    ci_bounds = np.array(ci_bounds)\n    ci_lower = predictions_smooth - ci_bounds\n    ci_upper = predictions_smooth + ci_bounds\n    ci_lower[np.where(ci_lower < 0)[0]] = 0\n    df_pred = pd.DataFrame({'pred_raw':predictions[winsize:], 'pred':predictions_smooth[winsize:],\n                          'obs':observations[winsize:], 'history':history[winsize:],\n                          'ci_lower':ci_lower[winsize:], 'ci_upper':ci_upper[winsize:],\n                          'pred_acc':acc_smooth[winsize:], 'pred_vel':vel_smooth[winsize:]},\n                           index=pred_dates[winsize:])\n    return df_pred", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Predict + plot functions"}, {"metadata": {}, "cell_type": "code", "source": "def test_plot(series, winsize, kf_type, params=None, title='', y_lims=None):\n    \"\"\"\n    Make a prediction and plot at once for convenience.\n    \n    \"\"\"\n    PLOT_RAW = True\n    SHOW_TRAJECTORY = True\n    DO_FORECAST = True\n    \n    result = kalman_test(series, winsize, kf_type, params)\n    \n    if DO_FORECAST:\n        forecast = kalman_forecast(series, winsize, kf_type, params)\n    \n    r2, mae, rmse = get_stats(result.obs, result.pred)\n    #r2, mae, rmse = get_stats(result.obs, result.pred_raw)\n    \n    plt.figure(figsize=[13,4])\n    plt.plot(series.index, series, 'o', color='g', linewidth=3)\n    plt.plot(result.index, result.pred, '-', color=(1,0,0,0.8), linewidth=3)\n    if PLOT_RAW:\n        plt.plot(result.index, result.pred_raw, 'o', color=(1,0,0,0.1), linewidth=3)\n        if DO_FORECAST:\n            plt.plot(forecast.index, forecast.pred_raw, 'o', color=(1,0,0,0.1), linewidth=3)\n    plt.fill_between(result.index, result.ci_lower, result.ci_upper, color=(1,0,0,0.1))\n    if DO_FORECAST:\n        plt.plot(forecast.index, forecast.pred, color='r', linewidth=3)\n        plt.fill_between(forecast.index, forecast.ci_lower, forecast.ci_upper, color=(1,0,0,0.1))\n    if SHOW_TRAJECTORY:\n        for df_h in result.history:\n            plt.plot(df_h.index, df_h.pred, '-', color=(0,0,1,0.2))\n    if y_lims:\n        plt.ylim(y_lims)\n    plt.xticks(rotation=45, fontsize='small')\n    plt.title(r'%s ($r^2$:%.3f, mae:%d, rmse:%d=%.1f%% of global max %d)' % (title, r2, mae, rmse, rmse * 100 / series.max(), series.max()))\n    plt.legend(['True', 'Prediction'])\n    plt.grid(True, 'both')\n    plt.tight_layout()\n    \n    plt.figure(figsize=[13,2])\n    plt.plot(series.index, [0] * len(series.index), 'k')\n    if DO_FORECAST:    \n        plt.plot(forecast.index, [0]*len(forecast.index), 'k')\n    plt.plot(result.index, result.pred_vel, '-', color=(0,0,1,0.8), linewidth=3)\n    plt.gca().axes.xaxis.set_ticklabels([])\n    plt.grid(True, 'both')\n    plt.title('Estimated growth rate')\n    plt.tight_layout()\n    \n    plt.figure(figsize=[13,2])\n    plt.plot(series.index, [0]*len(series.index), 'k')\n    if DO_FORECAST:\n        plt.plot(forecast.index, [0]*len(forecast.index), 'k')\n    plt.plot(result.index, result.pred_acc, '-', color=(0,0,1,0.8), linewidth=3)\n    plt.gca().axes.xaxis.set_ticklabels([])\n    plt.grid(True, 'both')\n    plt.title('Estimated growth acceleration')\n    plt.tight_layout()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def forecast_plot(series, winsize, kf_type, params=None, title='', y_lims=None):\n    \"\"\"\n    Make a prediction and plot at once for convenience.    \n    \"\"\"\n    PLOT_RAW = True\n    \n    forecast = kalman_forecast(series, winsize, kf_type, params)\n    plt.figure(figsize=[13,5])\n    plt.plot(series.index, series, 'o', color='g', linewidth=3)\n    plt.plot(forecast.index, forecast.pred, '-', color=(1,0,0,0.9), linewidth=3)\n    if PLOT_RAW:\n        plt.plot(forecast.index, forecast.pred_raw, '.', color='r')\n    plt.fill_between(forecast.index, forecast.ci_lower, forecast.ci_upper, color=(1,0,0,0.1))\n    if y_lims:\n        plt.ylim(y_lims)\n    plt.xticks(rotation=45, fontsize='small')\n    plt.title(title)\n    plt.legend(['True', 'Prediction'])\n    plt.grid(True, 'both')\n    plt.tight_layout()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### ICU prediction up to several days in the future"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "target = 'total_in_icu'\n#kf_type = 'linear'\n#kf_params = {'kf_p':1, 'kf_r':4, 'kf_q':0.10, 'kf_a':1}\nkf_type = 'ensemble'\nkf_params = {'kf_p':1, 'kf_r':4, 'kf_q':0.1, 'kf_n':1000}\nwinsizes = [7]\nshow_trajectory = False\nfor winsize in winsizes:\n    title = 'Belgium: %d-day prediction of required ICU beds' % winsize\n    test_plot(df_hosp[target], winsize, kf_type, kf_params, title)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stringency.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Predicted Cases up to several days in the future"}, {"metadata": {}, "cell_type": "code", "source": "## TODO: Write a wrapper function for the final Dataframe generation. Imprtant for WW Dataset Generation ...", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "target = 'cases_total'\n\nwinsize = 6", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_cases = stringency[stringency['ISO']=='BEL'][['DATE', 'country_name', 'ISO', 'cases_new']]\n\nbelgium_cases.set_index('DATE', inplace = True)\n\nbelgium_cases.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# plt.rcParams[\"figure.figsize\"] = (10,6)\n\n# belgium_cases = interpolate_weekend(belgium_cases, 'cases_new', '2020-01-03')\n\n# plt.plot(belgium_cases['cases_new']) ", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "belgium_new_cases_preds = kalman_forecast(belgium_cases[target], winsize, 'ensemble', None)\n\nbelgium_new_cases_preds", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_new_cases_test = kalman_test(belgium_cases[target], winsize, 'ensemble')\n\nbelgium_new_cases_test.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_new_cases_preds = pd.concat([belgium_new_cases_test, belgium_new_cases_preds])\n\nbelgium_new_cases_preds.iloc[:,-20:]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_cases.index", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# kf_type = 'ensemble'\n# kf_params = {'kf_p':1, 'kf_r':4, 'kf_q':0.1, 'kf_n':1000}\n# winsizes = [6]\n# show_trajectory = True\n# for winsize in winsizes:\n#     title = 'Belgium: %d-day prediction of Covid-19 Cases' % winsize\n#     test_plot(belgium_cases[target], winsize, kf_type, kf_params, title)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "belgium_df = belgium_new_cases_preds[['pred', 'ci_lower', 'ci_upper', 'pred_acc', 'pred_vel']]\n\nbelgium_df['country'] = 'belgium'\n\nbelgium_df.reset_index(inplace = True)\n\nbelgium_df.rename(columns = {'index' : 'DATE'}, inplace=True)\n\nbelgium_df['DATE'] = pd.to_datetime(belgium_df['DATE'], format =\"%Y-%m-%d\")\n\nbelgium_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stringency_belgium = stringency[['DATE', 'ISO', 'npi_stringency_index', 'tests_new_per_thousand', 'stats_population_density' , 'stats_population_urban',  'country_name']]\n\nstringency_belgium = stringency_belgium[stringency_belgium['ISO']=='BEL'].drop_duplicates()\n\nstringency_belgium.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_risk_df.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_risk_df = belgium_df.merge(stringency_belgium, left_on = 'DATE' , right_on = 'DATE', how = 'left')\n\n# assert belgium_risk_df.shape[0] == belgium_df.shape[0]\n\nbelgium_risk_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "belgium_risk_df", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "belgium_risk_df['npi_stringency_index'].fillna(method='ffill', inplace = True)\n\nbelgium_risk_df['infections_var'] = belgium_risk_df['pred'].diff()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "belgium_risk_df", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from geopy.geocoders import Nominatim\n# geolocator = Nominatim(user_agent=\"my_geocoder\")\n# location = geolocator.geocode(\"Belgium\")\n# belgium_risk_df['latitude'] = location.latitude\n# belgium_risk_df['longitude'] = location.longitude\n\n# belgium_risk_df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# from math import exp\n\n# def risk_index(cases_acceleration, npi_stringency_index, **kwargs):\n    \n#     raw_index =  (0.6 * cases_acceleration - 0.4 * (npi_stringency_index/100))\n    \n#     return 1 / (1 + exp(-raw_index))\n\n# ## The lower the stringency idenx --> Higher risk \n# ## Mobility Data ? \n\n\n# def discrete_risk_idx(risk_index):\n    \n#     if risk_index < 0.3:\n        \n#         return \"low risk\"\n    \n#     elif risk_index < 0.5:\n            \n#             return 'moderate risk'\n    \n#     else:\n        \n#         return \"high risk\"\n\n# def generate_predictions(iso_country, target, winsize):\n    \n#     '''\n    \n#     '''\n    \n#     print(\"Processing {} Data\".format(iso_country))\n    \n#     print(\"=\"*50)\n\n    \n#     ## Extract Target Data from Main Dataframe\n    \n#     columns = ['DATE', 'country_name', 'ISO'] + [target]\n    \n#     filtered_data  = stringency[stringency['ISO']==iso_country][columns]\n    \n#     filtered_data.set_index('DATE', inplace = True)\n    \n#     ## Train KF and Predict History , Predict Future\n    \n#     print(\"Training KF and Predicting {}, {} days ahead\".format(target, winsize))\n    \n#     print(\"-\"*32)\n    \n#     predict_hist = kalman_test(filtered_data[target], winsize, 'ensemble')\n    \n#     predict_future = kalman_forecast(filtered_data[target], winsize, 'ensemble', None)\n\n#     kf_predictions_all = pd.concat([predict_hist, predict_future])\n    \n#     kf_predictions_all = kf_predictions_all[['pred', 'ci_lower', 'ci_upper', 'pred_acc', 'pred_vel']]\n\n#     kf_predictions_all['country'] = iso_country\n\n#     kf_predictions_all.reset_index(inplace = True)\n\n#     kf_predictions_all.rename(columns = {'index' : 'DATE'}, inplace=True)\n\n#     kf_predictions_all['DATE'] = pd.to_datetime(kf_predictions_all['DATE'], format =\"%Y-%m-%d\")\n\n#     print(\"Reading and merging with Stringency and Complementary Data\")\n    \n#     print(\"-\"*32)\n    \n#     stringency_data = stringency[stringency['ISO']==iso_country].drop_duplicates()\n    \n#     stringency_data = stringency_data[['DATE', 'npi_stringency_index', 'tests_new_per_thousand', 'stats_population_density' , 'stats_population_urban',  'stats_population']]\n\n#     stringency_merged_df  = kf_predictions_all.merge(stringency_data, left_on = 'DATE' , right_on = 'DATE', how = 'left')\n\n#     print(\"Extrapolating Stringency Indices and Static Data\")\n    \n#     print(\"-\"*32)\n\n#     # assert belgium_risk_df.shape[0] == belgium_df.shape[0]\n    \n#     stringency_merged_df['npi_stringency_index'].fillna(method='ffill', inplace = True)\n    \n#     stringency_merged_df['stats_population_density'].fillna(method='ffill', inplace = True)\n    \n#     stringency_merged_df['stats_population_urban'].fillna(method='ffill', inplace = True)\n    \n#     stringency_merged_df['stats_population'].fillna(method='ffill', inplace = True)\n\n#     stringency_merged_df['infections_var'] = stringency_merged_df['pred'].pct_change()\n    \n#     print(\"Calculating the Risk Index\")\n    \n#     print(\"-\"*32)\n\n#     stringency_merged_df['risk_index'] = stringency_merged_df[['infections_var', 'npi_stringency_index']].apply(lambda x: risk_index(x[0], x[1]), axis =1)\n    \n#     stringency_merged_df['risk_index_disc'] = stringency_merged_df['risk_index'].apply(lambda x: discrete_risk_idx(x))\n\n#     print(\"Adding Geolocation Data\")\n    \n#     print(\"-\"*32)\n\n#     geolocator = Nominatim(user_agent = \"my_geocoder\")\n    \n#     location = geolocator.geocode(iso_country)\n    \n#     stringency_merged_df['latitude'] = location.latitude\n    \n#     stringency_merged_df['longitude'] = location.longitude\n    \n#     return stringency_merged_df\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "france_df = generate_predictions('FRA', 'cases_total', 6)\n\nbelgium_df = generate_predictions('BEL', 'cases_total', 6)\n\ngermany_df = generate_predictions('DEU', 'cases_total', 6)\n\nengland_df = generate_predictions('GBR', 'cases_total', 6)\n\nspain_df = generate_predictions('ESP', 'cases_total', 6)\n\nitaly_df = generate_predictions('ITA', 'cases_total', 6)\n\nmorocco_df = generate_predictions('MAR', 'cases_total', 6)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "france_df.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "belgium_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(stringency[stringency['ISO']=='FRA']['cases_total'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_all = pd.concat([france_df, belgium_df, germany_df, england_df, spain_df, italy_df, morocco_df])\n\ndf_all.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_all[(df_all['country'] == 'GBR')]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "france_cases = stringency[stringency['ISO']=='FRA'][['DATE', 'country_name', 'ISO', 'cases_new']]\n\nfrance_cases.set_index('DATE', inplace = True)\n\nfrance_cases.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "france_new_cases_preds = kalman_forecast(france_cases[target], winsize, 'ensemble', None)\n\nfrance_new_cases_preds", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "france_new_cases_test = kalman_test(france_cases[target], winsize, 'ensemble')\n\nfrance_new_cases_test.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Regional and Departmental Risk Index"}, {"metadata": {}, "cell_type": "markdown", "source": "### Reading UK Data at the Regional and the ULTA level"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "%%time\n\nimport requests\n\n## Reading UK Regional and LTLA Reported Cases, source: https://coronavirus.data.gov.uk/\n\nfilename = 'https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv'\n\nr = requests.get(filename, stream=True)\n\nuk_regional_cov19 = pd.read_csv(r.raw)\n\n\n## Reading the Health Life Expectancy for 65+ population , \n## https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthandlifeexpectancies/datasets/healthstatelifeexpectancyatbirthandatage65bylocalareasuk\n\nxls = pd.ExcelFile('/project_data/data_asset/hsleatbirthandatage65byukla201618.xlsx')\n\nuk_regional_hle_males = pd.read_excel(xls, 'HE - Male at 65', skiprows = 3)\n\nuk_regional_hle_males['sex'] = 'male'\n\nuk_regional_hle_females = pd.read_excel(xls, 'HE - Female at 65', skiprows = 3)\n\nuk_regional_hle_females['sex'] = 'female'\n\nuk_regional_hle = pd.concat([uk_regional_hle_males, uk_regional_hle_females], axis = 0)\n\n## Reading the UK Pop. Stats\n\nxls = pd.ExcelFile('/project_data/data_asset/ukmidyearestimates20192020ladcodes.xls')\n\nuk_pop_stats = pd.read_excel(xls, 'MYE2 - Persons', skiprows = 4)\n\n## Reading UK Health Systems\n\nuk_healthcare_system = pd.read_csv('/project_data/data_asset/20191015_stateofcare1819_ratingsdata.csv', encoding = 'latin')\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "uk_regional_hle[uk_regional_hle['Area Codes']  == 'E09000027']\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_pop_stats.rename(columns = {'+90': 90})\n\nage_cols = np.arange(65, 90)\n\nuk_pop_stats['pop_above_65'] = uk_pop_stats[age_cols].sum(axis = 1)\n\nuk_pop_stats.rename(columns = {'Geography1': 'area_type', 'Code': 'area_code', 'Name': 'area_name'}, inplace = True)\n\nuk_pop_stats = uk_pop_stats[['area_code', 'area_name', 'area_type', 'All ages', 'pop_above_65']]\n\nuk_pop_stats['area_name'] = uk_pop_stats['area_name'].apply(lambda z : z[0].upper() + z[1:].lower() if isinstance(z, str) else z)\n\nuk_pop_stats['area_type'] = uk_pop_stats['area_type'].apply(lambda z : z.upper() if isinstance(z, str) else z)\n\nuk_pop_stats.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_healthcare_system.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_regional_cov19.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_regional_cov19['Specimen date'] = pd.to_datetime(uk_regional_cov19['Specimen date'], format = \"%Y-%m-%d\")\n\nuk_regional_cov19.sort_values('Specimen date', ascending = True, inplace = True)\n\nuk_regional_cov19.set_index('Specimen date', inplace = True)\n\n# uk_regional_cov19['Area name'] = uk_regional_cov19['Area name'].apply(lambda x : x.upper())\n\nuk_regional_cov19['Area type'] = uk_regional_cov19['Area type'].apply(lambda x : x.upper())\n\nuk_regional_cov19['Area type'] = uk_regional_cov19['Area type'].apply(lambda x : 'COUNTRY' if x == 'NATION' else x)\n\nuk_regional_cov19.rename(columns = {'Area type': 'area_type', 'Area name': 'area_name', 'Area code': 'area_code'}, inplace = True)\n\nuk_regional_cov19.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_regional_cov19['area_type'].unique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stringency[stringency['ISO']=='GBR']", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# series = uk_regional_cov19[uk_regional_cov19['Area type']=='Nation']\n\n# predict_hist = kalman_test(series['Cumulative lab-confirmed cases'], 6, 'ensemble')\n\n# predict_future = kalman_forecast(series['Cumulative lab-confirmed cases'], 6, 'ensemble', None)\n\n# kf_predictions_all = pd.concat([predict_hist, predict_future])\n\n# kf_predictions_all = kf_predictions_all[['pred', 'ci_lower', 'ci_upper', 'pred_acc', 'pred_vel']]\n\n# kf_predictions_all.loc[:,'real_figures'] =  pd.Series(real_hist)\n\n# kf_predictions_all ", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from math import exp\n\ndef risk_index(growth_rate, npi_stringency_index, population_density, **kwargs):\n    \n    raw_index =  (0.5 * growth_rate  + 0.2 * (population_density/100) - 0.3 * (npi_stringency_index/100))\n    \n    raw_index = 100 * 1 / (1 + exp(-0.1 * raw_index))\n        \n    return raw_index\n\n\ndef discrete_risk_idx(risk_index):\n    \n    if risk_index < 20:\n        \n        return \"0 - 20\"\n    \n    elif risk_index < 50:\n            \n        return \"20 - 50\"\n    \n    elif risk_index < 70:\n            \n        return \"50 - 70\"\n        \n    elif risk_index < 90:\n            \n        return \"70 - 90\"\n        \n    else:\n        \n        return \"90 - 100\"\n    \n## stringency + mobility --> population's behaviour \n## WS2 --> regional's mood and awarness could be an input \n\n\ndef generate_predictions_granular(df, target, pop_stats, winsize, country_name, country_iso_code, area_type, area_name, geo_locate):\n    \n    '''\n    \n    '''\n    \n    print(\"Predicting {} Days ahead\".format(winsize))\n    \n    print(\"=\"*50)\n    \n    columns = ['area_name', 'area_type', 'area_code'] + [target]\n    \n    series = df[(df['area_name'] == area_name) & (df['area_type'] == area_type)][columns]\n    \n    print(\"Series Shape: {}\".format(series.shape))\n    \n    if len(series) > 0 :\n        \n       ## Train KF and Predict History , Predict Future\n\n        print(\"Training KF and Predicting {}, {} days ahead\".format(target, winsize))\n\n        print(\"-\"*32)\n\n        predict_hist = kalman_test(series[target], winsize, 'linear')\n\n        predict_future = kalman_forecast(series[target], winsize, 'linear', None)\n\n        kf_predictions_all = pd.concat([predict_hist, predict_future])\n\n        kf_predictions_all = kf_predictions_all[['pred', 'ci_lower', 'ci_upper', 'pred_acc', 'pred_vel']]\n        \n        kf_predictions_all.loc[:,'real_figures'] =  pd.Series(series[target])\n        \n        kf_predictions_all['country'] = country_name\n\n        kf_predictions_all['area_name'] = series['area_name'][0]\n\n        kf_predictions_all['area_type'] = series['area_type'][0]\n\n        kf_predictions_all.reset_index(inplace = True)\n\n        kf_predictions_all.rename(columns = {'index' : 'DATE'}, inplace=True)\n\n        kf_predictions_all['DATE'] = pd.to_datetime(kf_predictions_all['DATE'], format =\"%Y-%m-%d\")\n\n        print(\"Reading and merging with Stringency and Complementary Data\")\n\n        print(\"-\"*32)\n\n        stringency_data = stringency[stringency['ISO']==country_iso_code].drop_duplicates()\n\n        stringency_data = stringency_data[['DATE', 'npi_stringency_index', 'tests_new_per_thousand', 'stats_population_density' , 'stats_population_urban',  'stats_population', \n\n                                           'mobility_retail_recreation', 'mobility_grocery_pharmacy', 'mobility_parks', 'mobility_transit_stations', 'mobility_workplaces']]\n\n        stringency_merged_df  = kf_predictions_all.merge(stringency_data, left_on = 'DATE' , right_on = 'DATE', how = 'left')\n\n        print(\"Merging with Regional Population Data\")\n\n        print(\"-\"*32)\n        \n        if country_name != 'Italy':\n\n            area_code = series[(series['area_name'] == area_name)]['area_code'][0]  \n\n            if len(pop_stats[pop_stats['area_code'] == str(area_code)]) > 0:\n\n                if country_name == 'France': \n\n                     stringency_merged_df['area_population_density'] = int(pop_stats[(pop_stats['area_code']==str(area_code)) & (pop_stats['area_type'] == area_type)]['All ages'])*100.00/int(pop_stats[pop_stats['area_name'] == country_name]['All ages'].values[0])\n\n\n                elif country_name == 'England': \n\n                     stringency_merged_df['area_population_density'] = int(pop_stats[(pop_stats['area_code']==str(area_code))]['All ages'])*100.00/int(pop_stats[pop_stats['area_name'] == country_name]['All ages'].values[0])      \n\n\n            else:\n\n                stringency_merged_df['area_population_density'] = 0     \n        \n        else:\n            \n            print(\"Processing Exceptional Case of Italy\")\n            \n            print(\"-\"*32)\n            \n            if len(pop_stats[(pop_stats['area_name'] == area_name)]) > 0 :\n            \n                    stringency_merged_df['area_population_density'] = int(pop_stats[(pop_stats['area_name'] == area_name)]['All ages'])*100.00/int(pop_stats[pop_stats['area_name'] == country_name]['All ages'].values[0])\n            \n            else:\n                \n                   stringency_merged_df['area_population_density'] = 0 \n                    \n        print(\"Extrapolating Stringency Indices and Static Data\")\n\n        print(\"-\"*32)\n        \n        ### --> TBD: should be updated with Predictions instead\n\n        stringency_merged_df['npi_stringency_index'].fillna(method='ffill', inplace = True)\n\n        stringency_merged_df['stats_population_density'].fillna(method='ffill', inplace = True)\n\n        stringency_merged_df['stats_population_urban'].fillna(method='ffill', inplace = True)\n\n        stringency_merged_df['stats_population'].fillna(method='ffill', inplace = True)\n\n        stringency_merged_df['area_population_density'].fillna(method = 'ffill', inplace = True)\n\n    #     stringency_merged_df['infections_var'] = stringency_merged_df['pred'].pct_change()\n\n        print(\"Calculating the Risk Index\")\n\n        print(\"-\"*32)\n\n        stringency_merged_df['risk_index'] = stringency_merged_df[['pred_vel', 'npi_stringency_index', 'area_population_density']].apply(lambda x: risk_index(x[0], x[1], x[2]), axis =1)\n\n        stringency_merged_df['risk_index_disc'] = stringency_merged_df['risk_index'].apply(lambda x: discrete_risk_idx(x))\n\n        if geo_locate:\n            \n            print(\"Adding Geolocation Data\")\n\n            print(\"-\"*32)\n\n            geolocator = Nominatim(user_agent = \"my_geocoder\")\n\n            location = geolocator.geocode(area_name)\n\n            stringency_merged_df['latitude'] = location.latitude\n\n            stringency_merged_df['longitude'] = location.longitude\n        \n        else:\n            \n            stringency_merged_df['latitude'] = None\n\n            stringency_merged_df['longitude'] = None\n\n\n        return stringency_merged_df\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# columns = ['Area name', 'Area type', 'Area code'] + ['Cumulative lab-confirmed cases']\n\n# series = series[series['Area type']!='Lower tier local authority']\n\n# series = uk_regional_data[uk_regional_data['Area name']=='Kent'][columns]\n\n# dates = series.index\n\n# series['Cumulative lab-confirmed cases'][dates[0]]\n\n\n# generate_predictions_granular(uk_regional_data, 'Cumulative lab-confirmed cases', 6, 'ENGLAND', 'GBR', 'Buckinghamshire')", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# (series.reset_index().groupby(['Specimen date', 'Area name', 'Area type', 'Area code'], as_index=False).apply(lambda x: x if len(x)==1 else x.iloc[[-2]])\n\n#    .reset_index(level=0, drop=True))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Processing Risk Index for England"}, {"metadata": {}, "cell_type": "code", "source": "uk_regional_cov19.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# int(uk_pop_stats[uk_pop_stats['area_name'] == 'ENGLAND']['All ages'].values[0])\n\n# series = uk_regional_cov19[uk_regional_cov19['Area type'] == 'NATION']\n\n# area_code = series[(series['Area name'] == 'ENGLAND')]['Area code'][0]  \n\n# uk_pop_stats[(uk_pop_stats['area_code']==str(area_code)) & (uk_pop_stats['area_type'] == 'NATION')]['All ages']\n\nuk_pop_stats[uk_pop_stats['area_code'] == 'E92000001']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "uk_regional_cov19.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Sanity Chack for EAST MIDLANDS", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "%%time\n\n# area_types_uk = uk_regional_data['Area type'].unique()\n\narea_types_uk = ['COUNTRY', 'REGION', 'UPPER TIER LOCAL AUTHORITY']\n\ngeo_locate = True\n\nall_areas_uk_risk_index = pd.DataFrame()\n\nfor area_type_i in area_types_uk: \n    \n    uk_regional_data_i = uk_regional_cov19[uk_regional_cov19['area_type'] == area_type_i]\n    \n    geo_list = uk_regional_data_i['area_name'].unique()\n    \n    all_areas_names_uk = pd.DataFrame()\n    \n    for ii in geo_list:\n        \n        print(\"Generating risk index for: {} - {}\".format(area_type_i, ii))\n\n        kf_predictions_all = generate_predictions_granular(uk_regional_data_i, 'Cumulative lab-confirmed cases', uk_pop_stats, 6, 'England', 'GBR', area_type_i, ii, geo_locate)\n\n        all_areas_names_uk = all_areas_names_uk.append(kf_predictions_all)\n    \n    all_areas_uk_risk_index = all_areas_uk_risk_index.append(all_areas_names_uk)\n                \nall_areas_uk_risk_index.head()\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "all_areas_uk_risk_index[all_areas_uk_risk_index['area_name'] == 'Leicester']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "all_areas_uk_risk_index.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# length = np.arange(all_areas_uk_risk_index.shape[0])\n\nplt.hist(all_areas_uk_risk_index[['risk_index']].to_numpy().flatten())", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# !pip install folium \n\nimport folium\n\nregions = uk_regional_data['Area name'].unique()\n\ngeo_loc = []\n\nfor i in regions:\n    \n    geolocator = Nominatim(user_agent = \"my_geocoder\")\n    \n    location = geolocator.geocode(i)\n    \n    geo_loc.append([location.latitude, location.longitude])\n    \n\nmap = folium.Map(location=[38.9, -77.05], zoom_start=12)\n\nfor point in range(0, len(geo_loc)):\n    \n    folium.Marker(geo_loc[point], popup = geo_loc[point]).add_to(map)\n\nmap", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.figure(figsize=[13,4])\nplt.plot(series.index, series['Cumulative lab-confirmed cases'], 'o', color='g', linewidth=3)\nplt.plot(predictions.index, predictions.pred, '-', color=(1,0,0,0.8), linewidth=3)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Processing France Data\n\n\n"}, {"metadata": {}, "cell_type": "code", "source": "codes_officiel_france = pd.read_csv('/project_data/data_asset/code-officiel-geographique-2019-regions-et-departement (1).csv', sep = \";\")\n\ncodes_officiel_france.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Reading Infections Data\n\nfr_infections_regions = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/ad09241e-52fa-4be8-8298-e5760b43cae2', sep = \";\")\n\nfr_infections_regions_agg = fr_infections_regions[['reg', 'jour', 'P']].groupby(['reg', 'jour'])['P'].sum().reset_index()\n\nfr_infections_regions_agg = fr_infections_regions_agg.merge(codes_officiel_france[['Code INSEE R\u00e9gion', 'Nom r\u00e9gion']], left_on = 'reg', right_on = 'Code INSEE R\u00e9gion')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_infections_regions_agg.rename(columns={'Nom R\u00e9gion': 'area_name', 'Code INSEE R\u00e9gion': 'area_code', 'jour': 'Specimen date'}, inplace = True)\n\nfr_infections_regions_agg.drop('reg', inplace = True, axis =1 )\n\nfr_infections_regions_agg['area_type'] = 'Region'\n\nprint(\"The dataset contains {} duplicates\".format(fr_infections_regions_agg.duplicated().sum()))\n\nfr_infections_regions_agg.drop_duplicates(inplace = True)\n\nfr_infections_regions_agg.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_infections_dep = pd.read_csv('/project_data/data_asset/donnees-hospitalieres-covid19-2020-06-30-19h00.csv', sep = ';')\n\nfr_infections_dep_agg = fr_infections_dep[['dep', 'jour', 'hosp']].groupby(['dep', 'jour'])['hosp'].sum().reset_index()\n\nfr_infections_dep_agg = fr_infections_dep_agg.merge(codes_officiel_france[['Code INSEE D\u00e9partement', 'Nom D\u00e9partement majuscule']], left_on = 'dep', right_on = 'Code INSEE D\u00e9partement')\n\nfr_infections_dep_agg['Area type'] = 'DEPARTMENT'\n\nprint(\"The dataset contains {} duplicates\".format(fr_infections_dep_agg.duplicated().sum()))\n\nfr_infections_dep_agg.drop_duplicates(inplace = True)\n\nfr_infections_dep_agg.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_infections_dep_agg.rename(columns={'Nom D\u00e9partement majuscule': 'Area name', 'Code INSEE D\u00e9partement': 'Area code', 'jour': 'Specimen date'}, inplace = True)\n\nfr_infections_dep_agg.drop('dep', inplace = True, axis =1)\n\nfr_infections_dep_agg.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_infections_agg = pd.concat([fr_infections_regions_agg, fr_infections_dep_agg], axis =0 )\n\nfr_infections_agg.set_index('Specimen date', inplace=True)\n\nfr_infections_agg.index = pd.to_datetime(fr_infections_agg.index)\n\nfr_infections_agg.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "fr_infections_agg[(fr_infections_agg['Area name']=='ILE DE FRANCE') & (fr_infections_agg['Area type']=='REGION') ]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Reading Demographic Data\n\nxls = pd.ExcelFile('/project_data/data_asset/TCRD_021.xls')\n\nfr_regional_pop_data = pd.read_excel(xls, 'REG', skiprows = 3)\n\nfr_regional_pop_data['area_type'] = 'Region'\n\nfr_dep_pop_data = pd.read_excel(xls, 'DEP', skiprows = 3)\n\nfr_dep_pop_data['area_type'] = 'Department'\n\nfr_pop_data = pd.concat([fr_regional_pop_data, fr_dep_pop_data], axis = 0)\n\nfr_pop_data.rename(columns={'Unnamed: 0': 'area_code', 'Unnamed: 1': 'area_name', 'Ensemble': 'All ages', 'Part des 60 ans ou plus (en %)': 'pop_above_65'}, inplace = True)\n\nfr_pop_data = fr_pop_data[['area_code', 'area_name', 'area_type', 'All ages', 'pop_above_65']]\n\nfr_pop_data['area_code'] = fr_pop_data['area_code'].apply(str)\n\nfr_pop_data['area_name'] = fr_pop_data['area_name'].apply(lambda z : z.upper() if isinstance(z, str) else z)\n\nfr_pop_data['area_type'] = fr_pop_data['area_type'].apply(lambda z : z.upper() if isinstance(z, str) else z)\n\nfr_pop_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_pop_data['area_type'].unique()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "%%time\n\nfr_all_areas_preds = pd.DataFrame()\n\ngeo_locate = True\n\narea_types_all = fr_infections_agg['Area type'].unique()\n\nfor area_type in area_types_all:\n    \n    df_area_type = fr_infections_agg[fr_infections_agg['Area type'] == area_type]\n    \n    fr_area_preds = pd.DataFrame()\n        \n    area_names_all = df_area_type['Area name'].unique()\n    \n    for area_name in area_names_all:\n        \n        print(\"Generating risk index for: {} - {}\".format(area_type, area_name))  \n        \n        one_preds = generate_predictions_granular(df_area_type, 'hosp', fr_pop_data, 6, 'FRANCE', 'FRA', area_type, area_name, geo_locate)\n  \n        fr_area_preds = fr_area_preds.append(one_preds)\n    \n    fr_all_areas_preds = fr_all_areas_preds.append(fr_area_preds)\n        \nfr_all_areas_preds.head() \n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_all_areas_preds[fr_all_areas_preds['area_name']=='ILE DE FRANCE']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "series = fr_infections_agg[fr_infections_agg['Area type'] == 'Region']\n\narea_code = str(series[(series['Area name'] == 'ILE DE FRANCE') & (series['Area type'] == 'Region')]['Area code'][0])\n\narea_code\n\n# int(fr_pop_data[(fr_pop_data['area_code']==str(area_code)) & (fr_pop_data['area_type'] == area_type)]['All ages'])*100.00\n\n\n# fr_pop_data[fr_pop_data['area_code']=='11']\n\ngenerate_predictions_granular(series, 'hosp', fr_pop_data, 6, 'FRANCE', 'FRA', 'REGION', 'ILE DE FRANCE', True)\n\n# fr_pop_data[fr_pop_data['area_code']==area_code]['All ages']\n\n\n# fr_pop_data[(fr_pop_data['area_code']==str(area_code)) & (fr_pop_data['area_type'] == 'Region')]\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": " series = df[(df['Area name'] == area_name) & (df['Area type'] == area_type)][columns]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Data For Italy\n"}, {"metadata": {}, "cell_type": "code", "source": "italy_data_national = pd.read_csv('https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv')\n\nitaly_data_province = pd.read_csv('https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-province/dpc-covid19-ita-province.csv')\n\nitaly_data_regional = pd.read_csv('https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv')\n\nitaly_data_national.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_regional[italy_data_regional['denominazione_regione'] == 'Basilicata']\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province['denominazione_provincia'].unique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "italy_data_national_preds = italy_data_national[['data', 'stato', 'totale_casi']]\n\nitaly_data_national_preds.rename(columns = {'data': 'Specimen date', 'totale_casi': 'hosp'}, inplace = True)\n\nitaly_data_national_preds['Specimen date'] = pd.to_datetime(italy_data_national_preds['Specimen date'], format = \"%Y-%m-%d\")\n\nitaly_data_national_preds['Specimen date'] = italy_data_national_preds['Specimen date'].apply(lambda x : dt.datetime.strftime(x,  \"%Y-%m-%d\"))\n\nitaly_data_national_preds['area_code'] = None\n\nitaly_data_national_preds['area_name'] = 'ITALY'\n\nitaly_data_national_preds['area_type'] = 'COUNTRY'\n\nitaly_data_national_preds.drop('stato', inplace = True, axis = 1)\n\nitaly_data_national_preds.set_index('Specimen date', inplace = True)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_national_preds.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_national_preds['hosp'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_regional_preds = italy_data_regional[['data', 'stato', 'denominazione_regione', 'codice_regione', 'totale_casi']]\n\nitaly_data_regional_preds.rename(columns = {'data': 'Specimen date', 'totale_casi': 'hosp', 'denominazione_regione': 'area_name', 'codice_regione': 'area_code'}, inplace = True)\n\nitaly_data_regional_preds['Specimen date'] = pd.to_datetime(italy_data_regional_preds['Specimen date'], format = \"%Y-%m-%d\")\n\nitaly_data_regional_preds['Specimen date'] = italy_data_regional_preds['Specimen date'].apply(lambda x : dt.datetime.strftime(x,  \"%Y-%m-%d\"))\n\nitaly_data_regional_preds['area_type'] = 'REGION'\n\nitaly_data_regional_preds.drop('stato', inplace = True, axis = 1)\n\nitaly_data_regional_preds.set_index('Specimen date', inplace = True)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "italy_data_regional_preds.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_regional_preds[italy_data_regional_preds['Area name']=='Basilicata']['hosp'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province_preds = italy_data_province[['data', 'denominazione_provincia', 'totale_casi']].groupby(['data', 'denominazione_provincia'])['totale_casi'].sum().reset_index()\n\nitaly_data_province_preds.rename(columns = {'data': 'Specimen date', 'totale_casi': 'hosp', 'denominazione_provincia': 'area_name'}, inplace = True)\n\nitaly_data_province_preds['Specimen date'] = pd.to_datetime(italy_data_province_preds['Specimen date'], format = \"%Y-%m-%d\")\n\nitaly_data_province_preds['Specimen date'] = italy_data_province_preds['Specimen date'].apply(lambda x : dt.datetime.strftime(x,  \"%Y-%m-%d\"))\n\nitaly_data_province_preds['area_code'] = None\n\nitaly_data_province_preds['area_type'] = 'PROVINCE'\n\nitaly_data_province_preds.set_index('Specimen date', inplace = True)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_province_preds.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "italy_data_province_preds[italy_data_province_preds['Area name']=='In fase di definizione/aggiornamento']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_province[italy_data_province['denominazione_provincia'] == 'Siracusa']['totale_casi'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_province[italy_data_province['denominazione_provincia'] == 'Vicenza']['totale_casi'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_province_preds[italy_data_province_preds['Area name']=='Fuori Regione / Provincia Autonoma']['hosp'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(italy_data_province_preds[italy_data_province_preds['Area name']=='Chieti']['hosp'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_all_areas = pd.concat([italy_data_national_preds, italy_data_regional_preds], axis = 0)\n\nprint(\"The dataset contains {} duplicates\".format(italy_data_all_areas.duplicated().sum()))\n\nitaly_data_all_areas.drop_duplicates(inplace = True)\n\nitaly_data_all_areas['area_name'] = italy_data_all_areas['area_name'].apply(lambda z : z.upper())\n\nitaly_data_all_areas.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_all_areas['area_name'].unique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_all_areas['area_name'] = italy_data_all_areas['area_name'].apply(lambda x : 'BOLZANO' if x == 'P.A. BOLZANO' else 'TRENTO' if x == 'P.A. TRENTO' else x)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "italy_data_all_areas['area_name'].unique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Reading Demographic Data \n\nita_pop_data = pd.read_csv('/project_data/data_asset/DCIS_POPRES1_06072020030435663.csv')\n\nita_pop_data.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ita_pop_data[ita_pop_data['Territory'] == 'FRIULI VENEZIA GIULIA']", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "ita_pop_data = ita_pop_data[(ita_pop_data['Gender']=='total') & (ita_pop_data['STATCIV2']==99)][['Territory', 'Age', 'Value']].drop_duplicates()\n\nita_pop_data['All ages'] = ita_pop_data['Value'].groupby(ita_pop_data['Territory']).transform(max)\n\n# sub_data = ita_pop_data[(ita_pop_data['Territory']=='Italy') & (ita_pop_data['SEXISTAT1']==9) & (ita_pop_data['Gender']=='total') & (ita_pop_data['STATCIV2']==99)]\n\n# ita_pop_data['pop_above_65'] = sub_data['Value'].groupby(sub_data['Territory']).tail(10).sum()\n\nita_pop_data = ita_pop_data[['Territory', 'All ages']].drop_duplicates()\n\nita_pop_data.rename(columns= {'Territory': 'area_name'}, inplace = True)\n\nita_pop_data['area_name'] = ita_pop_data['area_name'].apply(lambda z : z.upper())\n\nita_pop_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "set(italy_data_all_areas['area_name']) & set(ita_pop_data['area_name'])", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "%%time\n\ngeo_locate = True \n\nita_all_areas_preds = pd.DataFrame()\n\narea_types_all = italy_data_all_areas['area_type'].unique()\n\nfor area_type in area_types_all:\n    \n    df_area_type = italy_data_all_areas[italy_data_all_areas['area_type'] == area_type]\n    \n    ita_area_preds = pd.DataFrame()\n    \n    area_names_all = df_area_type['area_name'].unique()\n    \n    for area_name in area_names_all:\n        \n        print(\"Generating risk index for: {} - {}\".format(area_type, area_name))\n        \n        one_preds = generate_predictions_granular(italy_data_all_areas, 'hosp', ita_pop_data, 6, 'ITALY', 'ITA', area_type, area_name, geo_locate)\n        \n        ita_area_preds = ita_area_preds.append(one_preds)\n    \n    ita_all_areas_preds = ita_all_areas_preds.append(ita_area_preds)\n        \nita_all_areas_preds.head()  \n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ita_all_areas_preds.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "generate_predictions_granular(italy_data_all_areas, 'hosp', ita_pop_data, 6, 'ITALY', 'ITA', area_type, area_name, geo_locate)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "italy_data_all_areas[italy_data_all_areas['Area name'] == 'In fase di definizione/aggiornamento']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Predictions for Germany\n"}, {"metadata": {}, "cell_type": "code", "source": "data_germany = pd.read_csv(\"/project_data/data_asset/df_pipeline_germany.csv\")\n\ndata_germany.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_germany.rename(columns = {'Cognos Area name': 'area_name', 'Area type': 'area_type', 'Cca 2': 'area_code'}, inplace = True)\n\ndata_germany = data_germany[[ 'area_name', 'area_type', 'cum cases', 'area_code']]\n\ndata_germany['area_code'] = data_germany['area_code'].apply(pd.to_numeric, errors='coerce')\n\ndata_germany.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "germ_pop_stats.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "germ_pop_stats = pd.read_csv('/project_data/data_asset/regional_population_germany.csv', sep = \";\")\n\ngerm_pop_stats.rename(columns = {'Schluessel': 'area_code', ' Name': 'area_name', ' Wert,,': 'population_density'}, inplace = True)\n\ngerm_pop_stats['population_density'] = germ_pop_stats['population_density'].apply(lambda x: x.replace(',', '')) \n\ngerm_pop_stats['population_density'] = germ_pop_stats['population_density'].apply(pd.to_numeric, errors='coerce')\n\ngerm_pop_stats.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "germ_pop_stats[' Name'].unique()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Merging All results in final DataFrame"}, {"metadata": {}, "cell_type": "code", "source": "# all_areas_uk_risk_index.reset_index(inplace = True)\n\n# fr_all_areas_preds.reset_index(inplace = True)\n\nrisk_index_world = pd.concat([all_areas_uk_risk_index, fr_all_areas_preds, ita_all_areas_preds], axis = 0)\n\nrisk_index_world.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "risk_index_world.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "all_areas_uk_risk_index.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fr_all_areas_preds.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ita_all_areas_preds.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist(risk_index_world[['risk_index']].to_numpy().flatten())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Write Data to CoS\n\nfrom project_lib import Project\n\nproject = Project.access()\n\nproject.save_data(file_name = \"risk_index_world.csv\",data = risk_index_world.to_csv(index = False), overwrite = True)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from project_lib import Project\n\nproject = Project.access()\n\nproject.save_data(file_name = \"risk_index_uk_only.csv\",data = all_areas_uk_risk_index.to_csv(index = False), overwrite = True)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}